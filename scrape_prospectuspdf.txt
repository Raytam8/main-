import os
import requests
import re
from bs4 import BeautifulSoup, NavigableString
import numpy
import pandas as pd

start_date_list = []
end_date_list = []
i = 2007
while i < 2026:
    if i == 2007: 
        start_date_list.append(f"{i}0625")
        end_date_list.append(f"{i}1231")
    elif i == 2025:
        start_date_list.append(f"{i}0101")
        end_date_list.append(f"{i}0701")
    else:
        start_date_list.extend([f"{i}0101", f"{i}0701"])
        end_date_list.extend([f"{i}0630" ,f"{i}1231"])
    i = i+1

start_date_list_b42007 = [20050101, 20060101, 20070101]
end_date_list_b42007 = [20051231, 20061231, 20070624]

def scrape_website(form_data): #scrape formal notice
    list_of_stock_codes = []
    list_of_release_time = []
    list_of_links = []
    list_of_filename = []
    url = 'https://www1.hkexnews.hk/search/titlesearch.xhtml?lang=en'
    print(1010)
    response = requests.post(url, data=form_data)
    soup = BeautifulSoup(response.content, 'html.parser')

    for tag in soup.find_all('div', class_='doc-link'):
        href = tag.find('a')['href']
        if href:
            if href[-3:] == 'pdf': 
                list_of_links.append("https://www1.hkexnews.hk" + href)
                file_name = href.split('/')[-1]
                list_of_filename.append(file_name)
            elif href[-3:] == 'htm':
                list_of_links.append("https://www1.hkexnews.hk" + href)
                file_name = (href.split('/')[-1])[:-4] + '.pdf'
                list_of_filename.append(file_name)                
            else:
                list_of_links.append('n/a not pdf type link')
                list_of_filename.append('n/a not pdf file type')
                print('unknwon document type')

    for td in soup.find_all('td', class_='text-right text-end stock-short-code'):
        children = [child for child in td.children if isinstance(child, NavigableString)]
        if len(children) != 0:
            list_of_stock_codes.append(children[-1].strip())
        else:
            list_of_stock_codes.append('n/a stock code cannot be found')
                    
    for td in soup.find_all('td', class_='text-right text-end release-time'):
        for child in td.children:
            if isinstance(child, NavigableString):
                try:
                    text = child.strip()
                    if text != '':
                        list_of_release_time.append(text)
                except Exception as e: 
                    list_of_release_time.append('n/a stock code cannot be found')

    return list_of_stock_codes, list_of_links, list_of_release_time, list_of_filename
    
def download_pdf(download_folder, list_of_links):
    print(1010)
    for link in list_of_links:
        if link.endswith('.pdf'):
            response = requests.get(link)
            file_name = link.split('/')[-1]
            with open(f"{download_folder}/{file_name}", 'wb') as f:
                f.write(response.content)
            print(f"Downloaded PDF: {file_name}")
        if link.endswith('htm'):
            response = requests.get(link)
            soup = BeautifulSoup(response.content, 'html.parser')
            font_tag = soup.find('a', string=lambda text: text and re.search(r'summary', text, re.IGNORECASE))
            if font_tag:
                htm = font_tag.get('href')
                if len(htm.split('/')) == 3:
                    response_htm = requests.get("/".join(link.split("/")[:-1]) + '/' + '/'.join(htm.split('/')[1:]))
                elif len(htm.split('/')) == 2:
                    response_htm = requests.get("/".join(link.split("/")[:-1]) + '/' + htm)
                else:
                    print('cannot download htm linked pdf')
                file_name = (link.split('/')[-1])[:-4] + '.pdf'

                with open(f"{download_folder}/{file_name}", 'wb') as f:
                    f.write(response_htm.content)
                print(f"Downloaded PDF: {file_name}")

def form_data_fn_af2007(start, end):
    form_data = {
    'lang': 'EN',  
    'category': '0',
    'market': 'SEHK',
    'searchType': '1',
    'documentType': '-1',
    't1code': '10000',
    't2Gcode': '5',
    't2code': '15200',
    'stockId': '-1',
    'from': f'{start}',
    'to': f'{end}',
    'MB-Daterange': '0',
    'title': ''      
    }
    return form_data

def form_data_fn_af2007_delisted(start, end):
    form_data = {
    'lang': 'EN',  
    'category': '1',
    'market': 'SEHK',
    'searchType': '1',
    'documentType': '-1',
    't1code': '10000',
    't2Gcode': '5',
    't2code': '15200',
    'stockId': '-1',
    'from': f'{start}',
    'to': f'{end}',
    'MB-Daterange': '0',
    'title': ''      
    }
    return form_data

def form_data_fn_b42007(start, end): #allotment results
    form_data = {
    'lang': 'EN',  
    'category': '0',
    'market': 'SEHK',
    'searchType': '2',
    'documentType': '10500',
    't1code': '',
    't2Gcode': '',
    't2code': '',
    'stockId': '',
    'from': f'{start}',
    'to': f'{end}',
    'MB-Daterange': '0',
    'title': ''      
    }
    return form_data

def form_data_fn_b42007_delisted(start, end): #allotment results
    form_data = {
    'lang': 'EN',  
    'category': '1',
    'market': 'SEHK',
    'searchType': '2',
    'documentType': '10500',
    't1code': '-2',
    't2Gcode': '-2',
    't2code': '-2',
    'stockId': '-1',
    'from': f'{start}',
    'to': f'{end}',
    'MB-Daterange': '0',
    'title': ''      
    }
    return form_data

download_folder = "./Downloaded_pdfs"

dfs= []
for i in range(len(start_date_list)): #after 20070625
    list_of_stock_codes, list_of_links, list_of_release_time, list_of_filename = scrape_website(form_data_fn_af2007(start_date_list[i], end_date_list[i])) 
    download_pdf(download_folder, list_of_links)
    list_of_stock_codes_dl, list_of_links_dl, list_of_release_time_dl, list_of_filename_dl = scrape_website(form_data_fn_af2007_delisted(start_date_list[i], end_date_list[i]))
    download_pdf(download_folder, list_of_links_dl)
    
    df_temp = pd.concat([pd.DataFrame({'stock_code':list_of_stock_codes, 'release_time':list_of_release_time, 'filename':list_of_filename}), pd.DataFrame({'stock_code':list_of_stock_codes_dl, 'release_time':list_of_release_time_dl, 'filename':list_of_filename_dl})], ignore_index=True).pipe(
        lambda df: df.assign(prospectus_date=pd.to_datetime(df['release_time'], format='%d/%m/%Y %H:%M').dt.normalize())
        ).drop(
            ['release_time'], axis=1
        ).pipe(
            lambda df: df.set_index(['stock_code', 'prospectus_date'])
        )

    dfs.append(df_temp)

df = pd.concat(dfs)
df.to_csv("./website_scrape_af2007.csv")

dfs = []
for i in range(len(start_date_list_b42007)):
    list_of_stock_codes, list_of_links, list_of_release_time, list_of_filename = scrape_website(form_data_fn_b42007(start_date_list_b42007[i], end_date_list_b42007[i]))
    download_pdf(download_folder, list_of_links)
    list_of_stock_codes_dl, list_of_links_dl, list_of_release_time_dl, list_of_filename_dl = scrape_website(form_data_fn_b42007_delisted(start_date_list_b42007[i], end_date_list_b42007[i]))
    download_pdf(download_folder, list_of_links)
    df_temp = pd.concat(
        [pd.DataFrame({'stock_code':list_of_stock_codes, 'release_time':list_of_release_time, 'filename':list_of_filename}), pd.DataFrame({'stock_code':list_of_stock_codes_dl, 'release_time':list_of_release_time_dl, 'filename':list_of_filename_dl})], ignore_index=True
        ).pipe(
        lambda df: df.assign(refund_date=pd.to_datetime(df['release_time'], format='%d/%m/%Y %H:%M').dt.normalize())
        ).drop(
            ['release_time'], axis=1
        ).pipe(
            lambda df: df.set_index(['stock_code', 'refund_date'])
        )

    dfs.append(df_temp)

df = pd.concat(dfs)
df.to_csv("./website_scrape_b42007.csv")
